{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition Prediction\n",
    "\n",
    "This is the fourth toy example from Jason Brownlee's [Long Short Term Memory Networks with Python](https://machinelearningmastery.com/lstms-with-python/). It demonstrates the solution to a sequence-to-sequence (aka seq2seq) prediction problem. Per section 9.3 of the book:\n",
    "\n",
    "> The problem is defined as calculating the sum output of two input numbers. This is\n",
    "challenging as each digit and mathematical symbol is provided as a character and the expected\n",
    "output is also expected as characters. For example, the input 10+6 with the output 16 would\n",
    "be represented by the sequences ['1', '0', '+', '6'] and ['1', '6'] respectively.\n",
    "\n",
    "> The model must learn not only the integer nature of the characters, but also the nature\n",
    "of the mathematical operation to perform. Notice how sequence is now important, and that\n",
    "randomly shuffling the input will create a nonsense sequence that could not be related to the\n",
    "output sequence. Also notice how the number of digits could vary in both the input and output\n",
    "sequences. Technically this makes the addition prediction problem a sequence-to-sequence\n",
    "problem that requires a many-to-many model to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "MODEL_FILE = os.path.join(DATA_DIR, \"torch-09-addition-predict-{:d}.model\")\n",
    "\n",
    "TRAIN_SIZE = 7500\n",
    "VAL_SIZE = 100\n",
    "TEST_SIZE = 500\n",
    "\n",
    "ENC_SEQ_LENGTH = 8\n",
    "DEC_SEQ_LENGTH = 2\n",
    "EMBEDDING_SIZE = 12\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We first generate a number of random math addition problems of the form a + b + c = d, then stringify them into the sequence of characters as needed by our network. We then left pad these sequences with space. Finally, we one-hot encode these padded strings so they can be fed into an LSTM. The blocks below show these transformations on a small set of random triples.\n",
    "\n",
    "We then apply these sequence of transformations to create our training and test sets. Each row of input to the network is represented by a sequence of one-hot vectors corresponding to the characters in the alphabet, and each row of output is a sequence of ids corresponding to the characters in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '+', '5', '+', '6'] ['1', '9']\n",
      "['9', '+', '1', '+', '5'] ['1', '5']\n",
      "['5', '+', '5', '+', '3'] ['1', '3']\n",
      "['4', '+', '4', '+', '1'] ['9']\n",
      "['3', '+', '6', '+', '0'] ['9']\n"
     ]
    }
   ],
   "source": [
    "def generate_random_addition_problems(max_num, num_probs):\n",
    "    lhs_1 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    lhs_2 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    lhs_3 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    rhs = lhs_1 + lhs_2 + lhs_3\n",
    "    in_seqs, out_seqs = [], []\n",
    "    for i in range(num_probs):\n",
    "        in_seqs.append([c for c in \"\".join([str(lhs_1[i]), \"+\", str(lhs_2[i]), \n",
    "                                            \"+\", str(lhs_3[i])])])\n",
    "        out_seqs.append([c for c in str(rhs[i])])\n",
    "    return in_seqs, out_seqs\n",
    "\n",
    "input_seq, output_seq = generate_random_addition_problems(10, 5)\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', '8', '+', '5', '+', '6'] ['1', '9']\n",
      "[' ', ' ', ' ', '9', '+', '1', '+', '5'] ['1', '5']\n",
      "[' ', ' ', ' ', '5', '+', '5', '+', '3'] ['1', '3']\n",
      "[' ', ' ', ' ', '4', '+', '4', '+', '1'] [' ', '9']\n",
      "[' ', ' ', ' ', '3', '+', '6', '+', '0'] [' ', '9']\n"
     ]
    }
   ],
   "source": [
    "def left_pad(chr_list, pad_len, pad_char=' '):\n",
    "    len_to_pad = pad_len - len(chr_list)\n",
    "    padded_list = []\n",
    "    for i in range(len_to_pad):\n",
    "        padded_list.append(pad_char)\n",
    "    padded_list.extend(chr_list)\n",
    "    return padded_list\n",
    "\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    print(left_pad(i, 8), left_pad(o, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', '8', '+', '5', '+', '6'] (8, 12) [1 9]\n",
      "[' ', ' ', ' ', '9', '+', '1', '+', '5'] (8, 12) [1 5]\n",
      "[' ', ' ', ' ', '5', '+', '5', '+', '3'] (8, 12) [1 3]\n",
      "[' ', ' ', ' ', '4', '+', '4', '+', '1'] (8, 12) [11  9]\n",
      "[' ', ' ', ' ', '3', '+', '6', '+', '0'] (8, 12) [11  9]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(padded_chr_list, char2idx):\n",
    "    encodeds = []\n",
    "    for c in padded_chr_list:\n",
    "        v = np.zeros(len(char2idx))\n",
    "        v[char2idx[c]] = 1\n",
    "        encodeds.append(v)\n",
    "    return np.array(encodeds)\n",
    "\n",
    "def one_hot_decode(enc_matrix, idx2char):\n",
    "    decodeds = []\n",
    "    for i in range(enc_matrix.shape[0]):\n",
    "        v = enc_matrix[i]\n",
    "        j = np.where(v == 1)[0][0]\n",
    "        decodeds.append(idx2char[j])\n",
    "    return decodeds\n",
    "\n",
    "chrs = [str(x) for x in range(10)] + ['+', ' ']\n",
    "char2idx, idx2char = {}, {}\n",
    "for i, c in enumerate(chrs):\n",
    "    char2idx[c] = i\n",
    "    idx2char[i] = c\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    X = one_hot_encode(left_pad(i, 8), char2idx)\n",
    "    Y = np.array([char2idx[x] for x in left_pad(o, 2)])\n",
    "    x_dec = one_hot_decode(X, idx2char)\n",
    "    print(x_dec, X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 8, 12) (7500, 2) (100, 8, 12) (100, 2) (500, 8, 12) (500, 2)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(data_size, enc_seqlen, dec_seqlen):\n",
    "    input_seq, output_seq = generate_random_addition_problems(10, data_size)\n",
    "    Xgen = np.zeros((data_size, enc_seqlen, EMBEDDING_SIZE))\n",
    "    Ygen = np.zeros((data_size, dec_seqlen))\n",
    "    for idx, (inp, outp) in enumerate(zip(input_seq, output_seq)):\n",
    "        Xgen[idx] = one_hot_encode(left_pad(inp, ENC_SEQ_LENGTH), char2idx)\n",
    "        Ygen[idx] = np.array([char2idx[x] for x in left_pad(outp, DEC_SEQ_LENGTH)])\n",
    "    return Xgen, Ygen\n",
    "\n",
    "Xtrain, Ytrain = generate_data(TRAIN_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "Xval, Yval = generate_data(VAL_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "Xtest, Ytest = generate_data(TEST_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "\n",
    "print(Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdditionPredictor (\n",
      "  (enc_lstm): LSTM(12, 75, batch_first=True)\n",
      "  (dec_lstm): LSTM(75, 50, batch_first=True)\n",
      "  (dec_fcn): Linear (50 -> 12)\n",
      "  (dec_softmax): Softmax ()\n",
      ")\n",
      "--- size debugging ---\n",
      "torch.Size([32, 2, 12])\n"
     ]
    }
   ],
   "source": [
    "class AdditionPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_seqlen, enc_embed_dim, enc_hidden_dim,\n",
    "                 dec_seqlen, dec_hidden_dim, output_dim):\n",
    "        super(AdditionPredictor, self).__init__()\n",
    "        # capture variables needed in forward\n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim\n",
    "        self.dec_seqlen = dec_seqlen\n",
    "        self.output_dim = output_dim\n",
    "        # define network layers\n",
    "        self.enc_lstm = nn.LSTM(enc_embed_dim, enc_hidden_dim, 1, batch_first=True)\n",
    "        self.dec_lstm = nn.LSTM(enc_hidden_dim, dec_hidden_dim, 1, batch_first=True)\n",
    "        self.dec_fcn = nn.Linear(dec_hidden_dim, output_dim)\n",
    "        self.dec_softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            he = (Variable(torch.randn(1, x.size(0), self.enc_hidden_dim).cuda()),\n",
    "                  Variable(torch.randn(1, x.size(0), self.enc_hidden_dim).cuda()))\n",
    "            hd = (Variable(torch.randn(1, x.size(0), self.dec_hidden_dim).cuda()),\n",
    "                  Variable(torch.randn(1, x.size(0), self.dec_hidden_dim).cuda()))\n",
    "        else:\n",
    "            he = (Variable(torch.randn(1, x.size(0), self.enc_hidden_dim)),\n",
    "                  Variable(torch.randn(1, x.size(0), self.enc_hidden_dim)))\n",
    "            hd = (Variable(torch.randn(1, x.size(0), self.dec_hidden_dim)),\n",
    "                  Variable(torch.randn(1, x.size(0), self.dec_hidden_dim)))\n",
    "\n",
    "        x, he = self.enc_lstm(x, he)         # encoder LSTM\n",
    "        x = x[:, -1, :].unsqueeze(1)         # encoder context vector\n",
    "        x = x.repeat(1, self.dec_seqlen, 1)  # repeat vector decoder seqlen times\n",
    "        x, hd = self.dec_lstm(x, hd)         # decoder LSTM\n",
    "        x_fcn = Variable(torch.zeros(x.size(0), self.dec_seqlen, self.output_dim))\n",
    "        for i in range(self.dec_seqlen):     # decoder LSTM -> fcn for each timestep\n",
    "            x_fcn[:, i, :] = self.dec_softmax(self.dec_fcn(x[:, i, :]))\n",
    "        x = x_fcn\n",
    "        return x\n",
    "\n",
    "model = AdditionPredictor(ENC_SEQ_LENGTH, EMBEDDING_SIZE, 75,\n",
    "                          DEC_SEQ_LENGTH, 50, len(chrs))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(model)\n",
    "\n",
    "# size debugging\n",
    "print(\"--- size debugging ---\")\n",
    "inp = Variable(torch.randn(BATCH_SIZE, ENC_SEQ_LENGTH, EMBEDDING_SIZE))\n",
    "outp = model(inp)\n",
    "print(outp.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20: loss=4.536, acc=0.055, val_loss=4.633, val_acc=0.042\n",
      "Epoch  2/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  3/20: loss=4.513, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  4/20: loss=4.513, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  5/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  6/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  7/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  8/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch  9/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 10/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 11/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 12/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 13/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 14/20: loss=4.512, acc=0.056, val_loss=4.633, val_acc=0.042\n",
      "Epoch 15/20: loss=4.488, acc=0.059, val_loss=4.583, val_acc=0.042\n",
      "Epoch 16/20: loss=4.481, acc=0.061, val_loss=4.585, val_acc=0.021\n",
      "Epoch 17/20: loss=4.471, acc=0.062, val_loss=4.576, val_acc=0.073\n",
      "Epoch 18/20: loss=4.469, acc=0.061, val_loss=4.586, val_acc=0.042\n",
      "Epoch 19/20: loss=4.466, acc=0.063, val_loss=4.576, val_acc=0.073\n",
      "Epoch 20/20: loss=4.466, acc=0.062, val_loss=4.583, val_acc=0.042\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(pred_var, true_var, idx2char):\n",
    "    if torch.cuda.is_available():\n",
    "        ypred = pred_var.cpu().data.numpy()\n",
    "        ytrue = true_var.cpu().data.numpy()\n",
    "    else:\n",
    "        ypred = pred_var.data.numpy()\n",
    "        ytrue = true_var.data.numpy()\n",
    "    pred_nums, true_nums = [], []\n",
    "    total_correct = 0\n",
    "    for i in range(ypred.shape[0]):\n",
    "        true_num = int(\"\".join([idx2char[x] for x in ytrue[i].tolist()]).lstrip())\n",
    "        true_nums.append(true_num)\n",
    "        try:\n",
    "            pred_num = int(\"\".join([idx2char[x] for x in ypred[i].tolist()]).lstrip())\n",
    "            pred_nums.append(pred_num)\n",
    "        except ValueError:\n",
    "            pred_nums.append(true_num + 1)\n",
    "            continue\n",
    "    return pred_nums, true_nums, accuracy_score(pred_nums, true_nums)\n",
    "\n",
    "history = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    num_batches = Xtrain.shape[0] // BATCH_SIZE\n",
    "    shuffled_indices = np.random.permutation(np.arange(Xtrain.shape[0]))\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for bid in range(num_batches):\n",
    "        \n",
    "        # extract one batch of data\n",
    "        Xbatch_data = Xtrain[shuffled_indices[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]]\n",
    "        Ybatch_data = Ytrain[shuffled_indices[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]]\n",
    "        Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "        Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "        if torch.cuda.is_available():\n",
    "            Xbatch = Xbatch.cuda()\n",
    "            Ybatch = Ybatch.cuda()\n",
    "            \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        loss = 0.\n",
    "        Ybatch_ = model(Xbatch)\n",
    "        for i in range(Ybatch.size(1)):\n",
    "            loss += loss_fn(Ybatch_[:, i, :], Ybatch[:, i])\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        _, ybatch_ = Ybatch_.max(2)\n",
    "        _, _, acc = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    # compute training loss and accuracy\n",
    "    train_loss /= num_batches\n",
    "    train_acc /= num_batches\n",
    "    \n",
    "    # compute validation loss and accuracy\n",
    "    val_loss, val_acc = 0., 0.\n",
    "    num_val_batches = Xval.shape[0] // BATCH_SIZE\n",
    "    for bid in range(num_val_batches):\n",
    "        # data\n",
    "        Xbatch_data = Xval[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "        Ybatch_data = Yval[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "        Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "        Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "        if torch.cuda.is_available():\n",
    "            Xbatch = Xbatch.cuda()\n",
    "            Ybatch = Ybatch.cuda()\n",
    "\n",
    "        loss = 0.\n",
    "        Ybatch_ = model(Xbatch)\n",
    "        for i in range(Ybatch.size(1)):\n",
    "            loss += loss_fn(Ybatch_[:, i, :], Ybatch[:, i])\n",
    "        val_loss += loss.data[0]\n",
    "\n",
    "        _, ybatch_ = Ybatch_.max(2)\n",
    "        _, _, acc = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "        val_acc += acc\n",
    "        \n",
    "    val_loss /= num_val_batches\n",
    "    val_acc /= num_val_batches\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_FILE.format(epoch+1))\n",
    "    print(\"Epoch {:2d}/{:d}: loss={:.3f}, acc={:.3f}, val_loss={:.3f}, val_acc={:.3f}\"\n",
    "          .format((epoch+1), NUM_EPOCHS, train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "    history.append((train_loss, val_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQLIEZYwxJpAsQRljjAmk/w+51LMuvSFZUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119586090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [x[0] for x in history]\n",
    "val_losses = [x[1] for x in history]\n",
    "accs = [x[2] for x in history]\n",
    "val_accs = [x[3] for x in history]\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(accs, color=\"r\", label=\"train\")\n",
    "plt.plot(val_accs, color=\"b\", label=\"valid\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(losses, color=\"r\", label=\"train\")\n",
    "plt.plot(val_losses, color=\"b\", label=\"valid\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = AdditionPredictor(ENC_SEQ_LENGTH, EMBEDDING_SIZE, 75,\n",
    "                                DEC_SEQ_LENGTH, 50, len(chrs))\n",
    "saved_model.load_state_dict(torch.load(MODEL_FILE.format(50)))\n",
    "if torch.cuda.is_available():\n",
    "    saved_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.052\n"
     ]
    }
   ],
   "source": [
    "ylabels, ypreds = [], []\n",
    "num_test_batches = Xtest.shape[0] // BATCH_SIZE\n",
    "for bid in range(num_test_batches):\n",
    "    Xbatch_data = Xtest[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "    Ybatch_data = Ytest[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "    Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "    Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "    if torch.cuda.is_available():\n",
    "        Xbatch = Xbatch.cuda()\n",
    "        Ybatch = Ybatch.cuda()\n",
    "\n",
    "    Ybatch_ = saved_model(Xbatch)\n",
    "    _, ybatch_ = Ybatch_.max(2)\n",
    "\n",
    "    pred_nums, true_nums, _ = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "    ylabels.extend(true_nums)\n",
    "    ypreds.extend(pred_nums)\n",
    "\n",
    "print(\"Test accuracy: {:.3f}\".format(accuracy_score(ylabels, ypreds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9+7+4    = 12 (expected 20)\n",
      "10+7+6   = 12 (expected 23)\n",
      "8+6+7    = 12 (expected 21)\n",
      "2+9+0    = 12 (expected 11)\n",
      "2+3+7    = 12 (expected 12)\n",
      "10+7+5   = 12 (expected 22)\n",
      "10+9+5   = 12 (expected 24)\n",
      "7+1+9    = 12 (expected 17)\n",
      "6+6+10   = 12 (expected 22)\n",
      "6+0+3    = 12 (expected  9)\n"
     ]
    }
   ],
   "source": [
    "Xbatch_data = Xtest[0:10]\n",
    "Ybatch_data = Ytest[0:10]\n",
    "Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "if torch.cuda.is_available():\n",
    "    Xbatch = Xbatch.cuda()\n",
    "    Ybatch = Ybatch.cuda()\n",
    "\n",
    "Ybatch_ = saved_model(Xbatch)\n",
    "_, ybatch_ = Ybatch_.max(2)\n",
    "\n",
    "pred_nums, true_nums, _ = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "Xbatch_var = Xbatch.data.numpy()\n",
    "\n",
    "for i in range(Xbatch_var.shape[0]):\n",
    "    problem = \"\".join(one_hot_decode(Xbatch_var[i], idx2char)).lstrip()\n",
    "    print(\"{:8s} = {:2d} (expected {:2d})\".format(problem, pred_nums[i], true_nums[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
