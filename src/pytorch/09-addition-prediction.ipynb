{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition Prediction\n",
    "\n",
    "This is the fourth toy example from Jason Brownlee's [Long Short Term Memory Networks with Python](https://machinelearningmastery.com/lstms-with-python/). It demonstrates the solution to a sequence-to-sequence (aka seq2seq) prediction problem. Per section 9.3 of the book:\n",
    "\n",
    "> The problem is defined as calculating the sum output of two input numbers. This is\n",
    "challenging as each digit and mathematical symbol is provided as a character and the expected\n",
    "output is also expected as characters. For example, the input 10+6 with the output 16 would\n",
    "be represented by the sequences ['1', '0', '+', '6'] and ['1', '6'] respectively.\n",
    "\n",
    "> The model must learn not only the integer nature of the characters, but also the nature\n",
    "of the mathematical operation to perform. Notice how sequence is now important, and that\n",
    "randomly shuffling the input will create a nonsense sequence that could not be related to the\n",
    "output sequence. Also notice how the number of digits could vary in both the input and output\n",
    "sequences. Technically this makes the addition prediction problem a sequence-to-sequence\n",
    "problem that requires a many-to-many model to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "MODEL_FILE = os.path.join(DATA_DIR, \"torch-09-addition-predict-{:d}.model\")\n",
    "\n",
    "TRAIN_SIZE = 7500\n",
    "VAL_SIZE = 100\n",
    "TEST_SIZE = 500\n",
    "\n",
    "ENC_SEQ_LENGTH = 8\n",
    "DEC_SEQ_LENGTH = 2\n",
    "EMBEDDING_SIZE = 12\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We first generate a number of random math addition problems of the form a + b + c = d, then stringify them into the sequence of characters as needed by our network. We then left pad these sequences with space. Finally, we one-hot encode these padded strings so they can be fed into an LSTM. The blocks below show these transformations on a small set of random triples.\n",
    "\n",
    "We then apply these sequence of transformations to create our training and test sets. Each row of input to the network is represented by a sequence of one-hot vectors corresponding to the characters in the alphabet, and each row of output is a sequence of ids corresponding to the characters in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '+', '4', '+', '2'] ['1', '0']\n",
      "['6', '+', '0', '+', '2'] ['8']\n",
      "['4', '+', '2', '+', '6'] ['1', '2']\n",
      "['6', '+', '1', '0', '+', '6'] ['2', '2']\n",
      "['8', '+', '3', '+', '7'] ['1', '8']\n"
     ]
    }
   ],
   "source": [
    "def generate_random_addition_problems(max_num, num_probs):\n",
    "    lhs_1 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    lhs_2 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    lhs_3 = np.random.randint(0, max_num + 1, num_probs)\n",
    "    rhs = lhs_1 + lhs_2 + lhs_3\n",
    "    in_seqs, out_seqs = [], []\n",
    "    for i in range(num_probs):\n",
    "        in_seqs.append([c for c in \"\".join([str(lhs_1[i]), \"+\", str(lhs_2[i]), \n",
    "                                            \"+\", str(lhs_3[i])])])\n",
    "        out_seqs.append([c for c in str(rhs[i])])\n",
    "    return in_seqs, out_seqs\n",
    "\n",
    "input_seq, output_seq = generate_random_addition_problems(10, 5)\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', '4', '+', '4', '+', '2'] ['1', '0']\n",
      "[' ', ' ', ' ', '6', '+', '0', '+', '2'] [' ', '8']\n",
      "[' ', ' ', ' ', '4', '+', '2', '+', '6'] ['1', '2']\n",
      "[' ', ' ', '6', '+', '1', '0', '+', '6'] ['2', '2']\n",
      "[' ', ' ', ' ', '8', '+', '3', '+', '7'] ['1', '8']\n"
     ]
    }
   ],
   "source": [
    "def left_pad(chr_list, pad_len, pad_char=' '):\n",
    "    len_to_pad = pad_len - len(chr_list)\n",
    "    padded_list = []\n",
    "    for i in range(len_to_pad):\n",
    "        padded_list.append(pad_char)\n",
    "    padded_list.extend(chr_list)\n",
    "    return padded_list\n",
    "\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    print(left_pad(i, 8), left_pad(o, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', '4', '+', '4', '+', '2'] (8, 12) [1 0]\n",
      "[' ', ' ', ' ', '6', '+', '0', '+', '2'] (8, 12) [11  8]\n",
      "[' ', ' ', ' ', '4', '+', '2', '+', '6'] (8, 12) [1 2]\n",
      "[' ', ' ', '6', '+', '1', '0', '+', '6'] (8, 12) [2 2]\n",
      "[' ', ' ', ' ', '8', '+', '3', '+', '7'] (8, 12) [1 8]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(padded_chr_list, char2idx):\n",
    "    encodeds = []\n",
    "    for c in padded_chr_list:\n",
    "        v = np.zeros(len(char2idx))\n",
    "        v[char2idx[c]] = 1\n",
    "        encodeds.append(v)\n",
    "    return np.array(encodeds)\n",
    "\n",
    "def one_hot_decode(enc_matrix, idx2char):\n",
    "    decodeds = []\n",
    "    for i in range(enc_matrix.shape[0]):\n",
    "        v = enc_matrix[i]\n",
    "        j = np.where(v == 1)[0][0]\n",
    "        decodeds.append(idx2char[j])\n",
    "    return decodeds\n",
    "\n",
    "chrs = [str(x) for x in range(10)] + ['+', ' ']\n",
    "char2idx, idx2char = {}, {}\n",
    "for i, c in enumerate(chrs):\n",
    "    char2idx[c] = i\n",
    "    idx2char[i] = c\n",
    "for i, o in zip(input_seq, output_seq):\n",
    "    X = one_hot_encode(left_pad(i, 8), char2idx)\n",
    "    Y = np.array([char2idx[x] for x in left_pad(o, 2)])\n",
    "    x_dec = one_hot_decode(X, idx2char)\n",
    "    print(x_dec, X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 8, 12) (7500, 2) (100, 8, 12) (100, 2) (500, 8, 12) (500, 2)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(data_size, enc_seqlen, dec_seqlen):\n",
    "    input_seq, output_seq = generate_random_addition_problems(10, data_size)\n",
    "    Xgen = np.zeros((data_size, enc_seqlen, EMBEDDING_SIZE))\n",
    "    Ygen = np.zeros((data_size, dec_seqlen))\n",
    "    for idx, (inp, outp) in enumerate(zip(input_seq, output_seq)):\n",
    "        Xgen[idx] = one_hot_encode(left_pad(inp, ENC_SEQ_LENGTH), char2idx)\n",
    "        Ygen[idx] = np.array([char2idx[x] for x in left_pad(outp, DEC_SEQ_LENGTH)])\n",
    "    return Xgen, Ygen\n",
    "\n",
    "Xtrain, Ytrain = generate_data(TRAIN_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "Xval, Yval = generate_data(VAL_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "Xtest, Ytest = generate_data(TEST_SIZE, ENC_SEQ_LENGTH, DEC_SEQ_LENGTH)\n",
    "\n",
    "print(Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdditionPredictor (\n",
      "  (enc_lstm): LSTM(12, 75, batch_first=True)\n",
      "  (dec_lstm): LSTM(75, 50, batch_first=True)\n",
      "  (dec_fcn): Linear (50 -> 12)\n",
      "  (dec_softmax): Softmax ()\n",
      ")\n",
      "--- size debugging ---\n",
      "torch.Size([32, 2, 12])\n"
     ]
    }
   ],
   "source": [
    "class AdditionPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_seqlen, enc_embed_dim, enc_hidden_dim,\n",
    "                 dec_seqlen, dec_hidden_dim, output_dim):\n",
    "        super(AdditionPredictor, self).__init__()\n",
    "        # capture variables needed in forward\n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim\n",
    "        self.dec_seqlen = dec_seqlen\n",
    "        self.output_dim = output_dim\n",
    "        # define network layers\n",
    "        self.enc_lstm = nn.LSTM(enc_embed_dim, enc_hidden_dim, 1, batch_first=True)\n",
    "        self.dec_lstm = nn.LSTM(enc_hidden_dim, dec_hidden_dim, 1, batch_first=True)\n",
    "        self.dec_fcn = nn.Linear(dec_hidden_dim, output_dim)\n",
    "        self.dec_softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            he = (Variable(torch.randn(1, x.size(0), self.enc_hidden_dim).cuda()),\n",
    "                  Variable(torch.randn(1, x.size(0), self.enc_hidden_dim).cuda()))\n",
    "            hd = (Variable(torch.randn(1, x.size(0), self.dec_hidden_dim).cuda()),\n",
    "                  Variable(torch.randn(1, x.size(0), self.dec_hidden_dim).cuda()))\n",
    "        else:\n",
    "            he = (Variable(torch.randn(1, x.size(0), self.enc_hidden_dim)),\n",
    "                  Variable(torch.randn(1, x.size(0), self.enc_hidden_dim)))\n",
    "            hd = (Variable(torch.randn(1, x.size(0), self.dec_hidden_dim)),\n",
    "                  Variable(torch.randn(1, x.size(0), self.dec_hidden_dim)))\n",
    "\n",
    "        x, he = self.enc_lstm(x, he)         # encoder LSTM\n",
    "        x = x[:, -1, :].unsqueeze(1)         # encoder context vector\n",
    "        x = x.repeat(1, self.dec_seqlen, 1)  # repeat vector decoder seqlen times\n",
    "        x, hd = self.dec_lstm(x, hd)         # decoder LSTM\n",
    "        x_fcn = Variable(torch.zeros(x.size(0), self.dec_seqlen, self.output_dim))\n",
    "        for i in range(self.dec_seqlen):     # decoder LSTM -> fcn for each timestep\n",
    "            x_fcn[:, i, :] = self.dec_softmax(self.dec_fcn(x[:, i, :]))\n",
    "        x = x_fcn\n",
    "        return x\n",
    "\n",
    "model = AdditionPredictor(ENC_SEQ_LENGTH, EMBEDDING_SIZE, 75,\n",
    "                          DEC_SEQ_LENGTH, 50, len(chrs))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(model)\n",
    "\n",
    "# size debugging\n",
    "print(\"--- size debugging ---\")\n",
    "inp = Variable(torch.randn(BATCH_SIZE, ENC_SEQ_LENGTH, EMBEDDING_SIZE))\n",
    "outp = model(inp)\n",
    "print(outp.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20: loss=4.603, acc=0.050, val_loss=4.447, val_acc=0.073\n",
      "Epoch  2/20: loss=4.515, acc=0.055, val_loss=4.446, val_acc=0.073\n",
      "Epoch  3/20: loss=4.515, acc=0.055, val_loss=4.446, val_acc=0.073\n",
      "Epoch  4/20: loss=4.512, acc=0.055, val_loss=4.449, val_acc=0.073\n",
      "Epoch  5/20: loss=4.473, acc=0.058, val_loss=4.452, val_acc=0.073\n",
      "Epoch  6/20: loss=4.464, acc=0.056, val_loss=4.454, val_acc=0.052\n",
      "Epoch  7/20: loss=4.462, acc=0.063, val_loss=4.454, val_acc=0.052\n",
      "Epoch  8/20: loss=4.462, acc=0.060, val_loss=4.453, val_acc=0.062\n",
      "Epoch  9/20: loss=4.462, acc=0.062, val_loss=4.455, val_acc=0.073\n",
      "Epoch 10/20: loss=4.463, acc=0.062, val_loss=4.456, val_acc=0.042\n",
      "Epoch 11/20: loss=4.462, acc=0.060, val_loss=4.453, val_acc=0.052\n",
      "Epoch 12/20: loss=4.462, acc=0.059, val_loss=4.455, val_acc=0.062\n",
      "Epoch 13/20: loss=4.462, acc=0.062, val_loss=4.455, val_acc=0.062\n",
      "Epoch 14/20: loss=4.462, acc=0.059, val_loss=4.453, val_acc=0.073\n",
      "Epoch 15/20: loss=4.462, acc=0.061, val_loss=4.456, val_acc=0.062\n",
      "Epoch 16/20: loss=4.463, acc=0.060, val_loss=4.455, val_acc=0.083\n",
      "Epoch 17/20: loss=4.462, acc=0.062, val_loss=4.454, val_acc=0.083\n",
      "Epoch 18/20: loss=4.462, acc=0.061, val_loss=4.456, val_acc=0.083\n",
      "Epoch 19/20: loss=4.462, acc=0.059, val_loss=4.454, val_acc=0.094\n",
      "Epoch 20/20: loss=4.462, acc=0.061, val_loss=4.456, val_acc=0.052\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(pred_var, true_var, idx2char):\n",
    "    if torch.cuda.is_available():\n",
    "        ypred = pred_var.cpu().data.numpy()\n",
    "        ytrue = true_var.cpu().data.numpy()\n",
    "    else:\n",
    "        ypred = pred_var.data.numpy()\n",
    "        ytrue = true_var.data.numpy()\n",
    "    pred_nums, true_nums = [], []\n",
    "    total_correct = 0\n",
    "    for i in range(ypred.shape[0]):\n",
    "        true_num = int(\"\".join([idx2char[x] for x in ytrue[i].tolist()]).lstrip())\n",
    "        true_nums.append(true_num)\n",
    "        try:\n",
    "            pred_num = int(\"\".join([idx2char[x] for x in ypred[i].tolist()]).lstrip())\n",
    "            pred_nums.append(pred_num)\n",
    "        except ValueError:\n",
    "            pred_nums.append(true_num + 1)\n",
    "            continue\n",
    "    return pred_nums, true_nums, accuracy_score(pred_nums, true_nums)\n",
    "\n",
    "history = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    num_batches = Xtrain.shape[0] // BATCH_SIZE\n",
    "    shuffled_indices = np.random.permutation(np.arange(Xtrain.shape[0]))\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for bid in range(num_batches):\n",
    "        \n",
    "        # extract one batch of data\n",
    "        Xbatch_data = Xtrain[shuffled_indices[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]]\n",
    "        Ybatch_data = Ytrain[shuffled_indices[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]]\n",
    "        Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "        Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "        if torch.cuda.is_available():\n",
    "            Xbatch = Xbatch.cuda()\n",
    "            Ybatch = Ybatch.cuda()\n",
    "            \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        loss = 0.\n",
    "        Ybatch_ = model(Xbatch)\n",
    "        for i in range(Ybatch.size(1)):\n",
    "            loss += loss_fn(Ybatch_[:, i, :], Ybatch[:, i])\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        _, ybatch_ = Ybatch_.max(2)\n",
    "        _, _, acc = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    # compute training loss and accuracy\n",
    "    train_loss /= num_batches\n",
    "    train_acc /= num_batches\n",
    "    \n",
    "    # compute validation loss and accuracy\n",
    "    val_loss, val_acc = 0., 0.\n",
    "    num_val_batches = Xval.shape[0] // BATCH_SIZE\n",
    "    for bid in range(num_val_batches):\n",
    "        # data\n",
    "        Xbatch_data = Xval[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "        Ybatch_data = Yval[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "        Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "        Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "        if torch.cuda.is_available():\n",
    "            Xbatch = Xbatch.cuda()\n",
    "            Ybatch = Ybatch.cuda()\n",
    "\n",
    "        loss = 0.\n",
    "        Ybatch_ = model(Xbatch)\n",
    "        for i in range(Ybatch.size(1)):\n",
    "            loss += loss_fn(Ybatch_[:, i, :], Ybatch[:, i])\n",
    "        val_loss += loss.data[0]\n",
    "\n",
    "        _, ybatch_ = Ybatch_.max(2)\n",
    "        _, _, acc = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "        val_acc += acc\n",
    "        \n",
    "    val_loss /= num_val_batches\n",
    "    val_acc /= num_val_batches\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_FILE.format(epoch+1))\n",
    "    print(\"Epoch {:2d}/{:d}: loss={:.3f}, acc={:.3f}, val_loss={:.3f}, val_acc={:.3f}\"\n",
    "          .format((epoch+1), NUM_EPOCHS, train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "    history.append((train_loss, val_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOecCyROUc865QPr/9IDD26YGSVsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1130f3110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [x[0] for x in history]\n",
    "val_losses = [x[1] for x in history]\n",
    "accs = [x[2] for x in history]\n",
    "val_accs = [x[3] for x in history]\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(accs, color=\"r\", label=\"train\")\n",
    "plt.plot(val_accs, color=\"b\", label=\"valid\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(losses, color=\"r\", label=\"train\")\n",
    "plt.plot(val_losses, color=\"b\", label=\"valid\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Network\n",
    "\n",
    "Our first test takes the (presumably) best trained model and evaluates it against the test set. The second test takes a slice of the test set and displays the predicted and the true result for each. From the second, it appears that the model has learned to always return a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = AdditionPredictor(ENC_SEQ_LENGTH, EMBEDDING_SIZE, 75,\n",
    "                                DEC_SEQ_LENGTH, 50, len(chrs))\n",
    "saved_model.load_state_dict(torch.load(MODEL_FILE.format(NUM_EPOCHS)))\n",
    "if torch.cuda.is_available():\n",
    "    saved_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.046\n"
     ]
    }
   ],
   "source": [
    "ylabels, ypreds = [], []\n",
    "num_test_batches = Xtest.shape[0] // BATCH_SIZE\n",
    "for bid in range(num_test_batches):\n",
    "    Xbatch_data = Xtest[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "    Ybatch_data = Ytest[bid * BATCH_SIZE : (bid + 1) * BATCH_SIZE]\n",
    "    Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "    Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "    if torch.cuda.is_available():\n",
    "        Xbatch = Xbatch.cuda()\n",
    "        Ybatch = Ybatch.cuda()\n",
    "\n",
    "    Ybatch_ = saved_model(Xbatch)\n",
    "    _, ybatch_ = Ybatch_.max(2)\n",
    "\n",
    "    pred_nums, true_nums, _ = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "    ylabels.extend(true_nums)\n",
    "    ypreds.extend(pred_nums)\n",
    "\n",
    "print(\"Test accuracy: {:.3f}\".format(accuracy_score(ylabels, ypreds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8+2+10 = 19 (expected 20)\n",
      "   9+0+4 = 17 (expected 13)\n",
      "   0+1+9 = 17 (expected 10)\n",
      "  10+8+4 = 19 (expected 22)\n",
      "   3+4+1 = 17 (expected  8)\n",
      "   0+7+4 = 11 (expected 11)\n",
      "   2+4+4 = 19 (expected 10)\n",
      "   3+3+9 = 19 (expected 15)\n",
      "   1+7+7 = 17 (expected 15)\n",
      "   7+6+6 = 19 (expected 19)\n"
     ]
    }
   ],
   "source": [
    "Xbatch_data = Xtest[0:10]\n",
    "Ybatch_data = Ytest[0:10]\n",
    "Xbatch = Variable(torch.from_numpy(Xbatch_data).float())\n",
    "Ybatch = Variable(torch.from_numpy(Ybatch_data).long())\n",
    "if torch.cuda.is_available():\n",
    "    Xbatch = Xbatch.cuda()\n",
    "    Ybatch = Ybatch.cuda()\n",
    "\n",
    "Ybatch_ = saved_model(Xbatch)\n",
    "_, ybatch_ = Ybatch_.max(2)\n",
    "\n",
    "pred_nums, true_nums, _ = compute_accuracy(ybatch_, Ybatch, idx2char)\n",
    "Xbatch_var = Xbatch.data.numpy()\n",
    "\n",
    "for i in range(Xbatch_var.shape[0]):\n",
    "    problem = \"\".join(one_hot_decode(Xbatch_var[i], idx2char)).lstrip()\n",
    "    print(\"{:>8s} = {:2d} (expected {:2d})\".format(problem, pred_nums[i], true_nums[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    os.remove(MODEL_FILE.format(i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
