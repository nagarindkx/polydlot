{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume Hybrid Keras/TF Model served by Tensorflow Serving\n",
    "\n",
    "This notebook shows the client code needed to consume a hybrid Keras-Tensorflow model served over Tensorflow Serving. The Tensorflow Serving Model Server needs to be started against our MNIST CNN test model at `EXPORT_DIR_ROOT/EXPORT_MODEL_NAME` using the following command: \n",
    "\n",
    "    bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \\\n",
    "        --port=9000 --model_name=mnist_cnn \\\n",
    "        --model_base_path=/home/sujit/Projects/polydlot/data/tf-export/mnist_cnn_model\n",
    "\n",
    "Code for the client is adapted from the [mnist_client.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py) code provided as part of the TF-Serving examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from grpc.beta import implementations\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import numpy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONCURRENCY = 1\n",
    "NUM_TESTS = 10\n",
    "SERVER_HOST = \"localhost\"\n",
    "SERVER_PORT = 9000\n",
    "WORK_DIR = \"/tmp\"\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"mnist_test.csv\")\n",
    "\n",
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_test.csv: 0 lines read\n",
      "mnist_test.csv: 10000 lines read\n",
      "(10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def parse_file(filename):\n",
    "    xdata, ydata = [], []\n",
    "    fin = open(filename, \"rb\")\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        if i % 10000 == 0:\n",
    "            print(\"{:s}: {:d} lines read\".format(\n",
    "                os.path.basename(filename), i))\n",
    "        cols = line.strip().split(\",\")\n",
    "        ydata.append(int(cols[0]))\n",
    "        xdata.append(numpy.reshape(numpy.array([float(x) / 255. \n",
    "            for x in cols[1:]]), (IMG_SIZE, IMG_SIZE, 1)))\n",
    "        i += 1\n",
    "    fin.close()\n",
    "    print(\"{:s}: {:d} lines read\".format(os.path.basename(filename), i))\n",
    "    y = numpy.array(ydata)\n",
    "    X = numpy.array(xdata)\n",
    "    return X, y\n",
    "\n",
    "Xtest, ytest = parse_file(TEST_FILE)\n",
    "print(Xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1) float32 (1,) int32\n"
     ]
    }
   ],
   "source": [
    "def datagen(X, y, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES):\n",
    "    ohe = OneHotEncoder(n_values=num_classes)\n",
    "    while True:\n",
    "        shuffled_indices = numpy.random.permutation(numpy.arange(len(y)))\n",
    "        num_batches = len(y) // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            batch_indices = shuffled_indices[bid*batch_size:(bid+1)*batch_size]\n",
    "            Xbatch = numpy.zeros((batch_size, X.shape[1], X.shape[2], X.shape[3]), \n",
    "                                 dtype=\"float32\")\n",
    "            Ybatch = numpy.zeros((batch_size,), dtype=\"int32\")\n",
    "            for i in range(batch_size):\n",
    "                Xbatch[i] = X[batch_indices[i]]\n",
    "                Ybatch[i] = y[batch_indices[i]]\n",
    "            yield Xbatch, Ybatch\n",
    "\n",
    "self_test_gen = datagen(Xtest, ytest, batch_size=1)\n",
    "Xbatch, Ybatch = self_test_gen.next()\n",
    "print(Xbatch.shape, Xbatch.dtype, Ybatch.shape, Ybatch.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holder classes\n",
    "\n",
    "TF-serving exposes an asynchronous interface, so there is no guarantee of the responses coming back in the same order as the request, so we need to build some additional infrastructure to handle that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _ResultCounter(object):\n",
    "    \"\"\" Counter for prediction results \"\"\"\n",
    "    def __init__(self, num_tests, concurrency):\n",
    "        self._num_tests = num_tests\n",
    "        self._concurrency = concurrency\n",
    "        self._error = 0\n",
    "        self._done = 0\n",
    "        self._active = 0\n",
    "        self._results = []\n",
    "        self._condition = threading.Condition()\n",
    "\n",
    "    def inc_error(self):\n",
    "        with self._condition:\n",
    "            self._error += 1\n",
    "            \n",
    "    def inc_done(self):\n",
    "        with self._condition:\n",
    "            self._done += 1\n",
    "            self._condition.notify()\n",
    "\n",
    "    def dec_active(self):\n",
    "        with self._condition:\n",
    "            self._active -= 1\n",
    "            self._condition.notify()\n",
    "    \n",
    "    def add_result(self, result):\n",
    "        with self._condition:\n",
    "            self._results.append(result)\n",
    "            self._condition.notify()\n",
    "            \n",
    "    def get_error_rate(self):\n",
    "        with self._condition:\n",
    "            while self._done != self._num_tests:\n",
    "                self._condition.wait()\n",
    "        return self._error / float(self._num_tests)\n",
    "\n",
    "    def throttle(self):\n",
    "        with self._condition:\n",
    "            while self._active == self._concurrency:\n",
    "                self._condition.wait()\n",
    "            self._active += 1\n",
    "\n",
    "\n",
    "def _create_rpc_callback(image, label, result_counter):\n",
    "    def _callback(result_future):\n",
    "        print(\"image\", image.shape, \"label\", label.shape)\n",
    "        exception = result_future.exception()\n",
    "        if exception:\n",
    "            result_counter.inc_error()\n",
    "            print(exception)\n",
    "        else:\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "        response = numpy.array(result_future.result().outputs['scores'].float_val)\n",
    "        prediction = numpy.argmax(response)\n",
    "        if label != prediction:\n",
    "            result_counter.inc_error()\n",
    "        result_counter.add_result((image, label, prediction))\n",
    "        result_counter.inc_done()\n",
    "        result_counter.dec_active()\n",
    "    return _callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image (28, 28, 1) label ()\n",
      "AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input tensor alias not found in signature: input. Inputs expected to be in the set {images}.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/site-packages/grpc/_channel.py\", line 731, in channel_spin\n",
      "    completed_call = event.tag(event)\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/site-packages/grpc/_channel.py\", line 187, in handle_event\n",
      "    callback()\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/site-packages/grpc/_channel.py\", line 328, in <lambda>\n",
      "    self._state.callbacks.append(lambda: fn(self))\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 139, in <lambda>\n",
      "    self._future.add_done_callback(lambda ignored_callback: fn(self))\n",
      "  File \"<ipython-input-5-13f422dba138>\", line 54, in _callback\n",
      "    response = numpy.array(result_future.result().outputs['scores'].float_val)\n",
      "  File \"/home/sujit/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 112, in result\n",
      "    raise _abortion_error(rpc_error_call)\n",
      "AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input tensor alias not found in signature: input. Inputs expected to be in the set {images}.\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_gen = datagen(Xtest, ytest, batch_size=1)\n",
    "\n",
    "channel = implementations.insecure_channel(SERVER_HOST, SERVER_PORT)\n",
    "stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "result_counter = _ResultCounter(NUM_TESTS, CONCURRENCY)\n",
    "tf.contrib.keras.backend.set_learning_phase(False)\n",
    "for i in range(NUM_TESTS):\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = \"mnist_cnn\"\n",
    "    request.model_spec.signature_name = \"predict\"\n",
    "    Xbatch, Ybatch = test_gen.next()\n",
    "    request.inputs[\"images\"].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(Xbatch[0], shape=Xbatch.shape))\n",
    "\n",
    "    result_counter.throttle()\n",
    "    result_future = stub.Predict.future(request, 5.0)\n",
    "    result_future.add_done_callback(_create_rpc_callback(Xbatch[0], Ybatch[0], result_counter))\n",
    "\n",
    "time.sleep(5)\n",
    "print(\"\\n---\")\n",
    "error_rate = result_counter.get_error_rate()\n",
    "print(\"Percent Error rate: {:.3f}\".format(error_rate * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
